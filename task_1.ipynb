{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "## This notebook implements and compares multiple MLP and CNN architectures on the **Fashion-MNIST** dataset using TensorFlow/Keras.\n",
    "It also retrains the top-3 best configurations on **CIFAR-10** to study generalization across datasets.\n",
    "The workflow includes model building, training, evaluation, logging, and visualization."
   ],
   "id": "ac06cc85a6460b75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, initializers, optimizers\n",
    "from tensorflow.keras.layers import (Input, Dense, Flatten, Dropout,\n",
    "                                     Conv2D, MaxPooling2D, BatchNormalization)\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                        CSVLogger, ReduceLROnPlateau)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "ad1073b5c86ec473"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setting random seeds for reproduciility and logging",
   "id": "d64eb94542dba812"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "set_seed(42)\n",
    "\n",
    "for p in [\"results\", \"results/models\", \"results/logs\", \"results/summaries\"]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ],
   "id": "9ea75937578fb9ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The helper functions for Regularizers, Initializers, Optimizers, and Keras Callbacks for early stopping and logging",
   "id": "d2c67ab192757ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_regularizer(cfg):\n",
    "    if cfg.get(\"regularizer\") == \"l1\":\n",
    "        return regularizers.l1(cfg.get(\"reg_value\", 1e-4))\n",
    "    elif cfg.get(\"regularizer\") == \"l2\":\n",
    "        return regularizers.l2(cfg.get(\"reg_value\", 1e-4))\n",
    "    return None\n",
    "\n",
    "def get_initializer(cfg):\n",
    "    name = cfg.get(\"initializer\")\n",
    "    return initializers.get(name) if name else None\n",
    "\n",
    "def get_optimizer(cfg):\n",
    "    opt = cfg.get(\"optimizer\", \"adam\").lower()\n",
    "    lr = cfg.get(\"learning_rate\", None)\n",
    "    if opt == \"adam\":\n",
    "        return optimizers.Adam(learning_rate=lr or 1e-3)\n",
    "    if opt == \"sgd\":\n",
    "        return optimizers.SGD(learning_rate=lr or 1e-2, momentum=0.9)\n",
    "    if opt == \"rmsprop\":\n",
    "        return optimizers.RMSprop(learning_rate=lr or 1e-3)\n",
    "    return optimizers.Adam()\n",
    "\n",
    "def callbacks_for(run_id):\n",
    "    return [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
    "        ModelCheckpoint(f\"results/models/{run_id}.h5\",\n",
    "                        monitor=\"val_accuracy\", save_best_only=True),\n",
    "        CSVLogger(f\"results/logs/{run_id}.csv\"),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5,\n",
    "                          patience=3, min_lr=1e-6)\n",
    "    ]"
   ],
   "id": "4bafa6f960138e09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Just another helper functions to make summaries and custom naming of model runid with configurations to keep track of all the runs.",
   "id": "d69e9e67e73bc040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def summarize_run(run_id, dataset, cfg, hist, test_acc):\n",
    "    best_val_acc = max(hist.history.get(\"val_accuracy\", [0.0]))\n",
    "    summary = {\n",
    "        \"run_id\": run_id,\n",
    "        \"dataset\": dataset,\n",
    "        \"config\": cfg,\n",
    "        \"best_val_accuracy\": float(best_val_acc),\n",
    "        \"final_test_accuracy\": float(test_acc)\n",
    "    }\n",
    "    with open(f\"results/summaries/{run_id}.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    return summary\n",
    "\n",
    "def make_run_id(model_type, cfg):\n",
    "    act = cfg.get(\"activation\", \"relu\")\n",
    "    opt = cfg.get(\"optimizer\", \"adam\")\n",
    "    init = cfg.get(\"initializer\", \"glorot_uniform\")\n",
    "    if model_type == \"mlp\":\n",
    "        arch = \"x\".join(map(str, cfg.get(\"layers\", [])))\n",
    "    else:\n",
    "        arch = \"x\".join(map(str, cfg.get(\"filters\", [])))\n",
    "    reg = cfg.get(\"regularizer\", \"none\")\n",
    "    return f\"{model_type}_{act}_{opt}_{init}_{reg}_{arch}\""
   ],
   "id": "ed19d7564a47973c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading and preprocessing of Fashion-MNIST dataset",
   "id": "243beff7da8cc0ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train.astype(\"float32\") / 255.0, x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "x_train_flat = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test_flat = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "x_train_flat, x_val_flat, y_train_flat, y_val_flat = train_test_split(\n",
    "    x_train_flat, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "x_train_cnn, x_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
    "    x_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "\n",
    "x_train_cnn = x_train_cnn[..., np.newaxis]\n",
    "x_val_cnn = x_val_cnn[..., np.newaxis]\n",
    "x_test_cnn = x_test[..., np.newaxis]"
   ],
   "id": "1f4e2e37111174d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Both CNN and MLP architectures with softmax activation and sparse_categorical_crossentropy",
   "id": "58cbb9309e078565"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_mlp(cfg):\n",
    "    model = Sequential()\n",
    "    init = get_initializer(cfg)\n",
    "    model.add(Input(shape=(784,)))\n",
    "    for units in cfg[\"layers\"]:\n",
    "        model.add(Dense(units,\n",
    "                        activation=cfg.get(\"activation\", \"relu\"),\n",
    "                        kernel_regularizer=get_regularizer(cfg),\n",
    "                        kernel_initializer=init))\n",
    "        if cfg.get(\"dropout\"):\n",
    "            model.add(Dropout(cfg[\"dropout\"]))\n",
    "    model.add(Dense(10, activation=\"softmax\", kernel_initializer=init))\n",
    "    model.compile(optimizer=get_optimizer(cfg),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_cnn(cfg, input_shape=(28,28,1)):\n",
    "    init = get_initializer(cfg)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    for f in cfg[\"filters\"]:\n",
    "        model.add(Conv2D(f, (cfg[\"kernel\"], cfg[\"kernel\"]),\n",
    "                         activation=cfg.get(\"activation\", \"relu\"),\n",
    "                         padding=\"same\",\n",
    "                         kernel_regularizer=get_regularizer(cfg),\n",
    "                         kernel_initializer=init))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(2))\n",
    "    if cfg.get(\"dropout\"):\n",
    "        model.add(Dropout(cfg[\"dropout\"]))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=cfg.get(\"activation\", \"relu\"),\n",
    "                    kernel_initializer=init))\n",
    "    model.add(Dense(10, activation=\"softmax\", kernel_initializer=init))\n",
    "    model.compile(optimizer=get_optimizer(cfg),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "id": "426715a5c7126e04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Different sets of hyperparameter configurations for MLP and CNN with combination of optimizers, and initializers",
   "id": "6e1ce205a8bac6a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_mlp_configs = [\n",
    "    {\"layers\":[256,128], \"activation\":\"relu\",\"regularizer\":None,\"dropout\":0.2},\n",
    "    {\"layers\":[512,256,128], \"activation\":\"relu\",\"regularizer\":\"l2\",\"reg_value\":1e-4,\"dropout\":0.3},\n",
    "    {\"layers\":[512,256], \"activation\":\"elu\",\"regularizer\":\"l1\",\"reg_value\":1e-5,\"dropout\":0.3},\n",
    "    {\"layers\":[512,256,128], \"activation\":\"relu\",\"regularizer\":None,\"dropout\":0.4}\n",
    "]\n",
    "\n",
    "base_cnn_configs = [\n",
    "    {\"filters\":[32,64], \"kernel\":3, \"activation\":\"relu\",\"regularizer\":None,\"dropout\":0.3},\n",
    "    {\"filters\":[32,64,128], \"kernel\":3, \"activation\":\"relu\",\"regularizer\":\"l2\",\"reg_value\":1e-4,\"dropout\":0.4},\n",
    "    {\"filters\":[32,64,128], \"kernel\":5, \"activation\":\"relu\",\"regularizer\":None,\"dropout\":0.4},\n",
    "    {\"filters\":[64,128], \"kernel\":3, \"activation\":\"elu\",\"regularizer\":None,\"dropout\":0.3}\n",
    "]\n",
    "\n",
    "optimizers_to_try = [\"adam\", \"sgd\", \"rmsprop\"]\n",
    "initializers_to_try = [\"glorot_uniform\", \"he_normal\"]\n",
    "\n",
    "mlp_configs, cnn_configs = [], []\n",
    "for base in base_mlp_configs:\n",
    "    for opt in optimizers_to_try:\n",
    "        for init in initializers_to_try:\n",
    "            cfg = dict(base)\n",
    "            cfg.update({\"optimizer\": opt, \"initializer\": init})\n",
    "            mlp_configs.append(cfg)\n",
    "for base in base_cnn_configs:\n",
    "    for opt in optimizers_to_try:\n",
    "        for init in initializers_to_try:\n",
    "            cfg = dict(base)\n",
    "            cfg.update({\"optimizer\": opt, \"initializer\": init})\n",
    "            cnn_configs.append(cfg)"
   ],
   "id": "8b2ec23253824a1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training on MLP and CNN models for Fashion-MNIST and storing the results and CSV",
   "id": "89c49d8d6def77c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "summaries = []\n",
    "\n",
    "for cfg in mlp_configs:\n",
    "    run_id = make_run_id(\"mlp\", cfg)\n",
    "    print(f\"\\nTraining {run_id} ...\")\n",
    "    model = build_mlp(cfg)\n",
    "    hist = model.fit(x_train_flat, y_train_flat,\n",
    "                     validation_data=(x_val_flat, y_val_flat),\n",
    "                     epochs=30, batch_size=128,\n",
    "                     callbacks=callbacks_for(run_id), verbose=2)\n",
    "    _, test_acc = model.evaluate(x_test_flat, y_test, verbose=0)\n",
    "    summaries.append(summarize_run(run_id, \"fmnist\", cfg, hist, test_acc))\n",
    "\n",
    "for cfg in cnn_configs:\n",
    "    run_id = make_run_id(\"cnn\", cfg)\n",
    "    print(f\"\\nTraining {run_id} ...\")\n",
    "    model = build_cnn(cfg, input_shape=(28,28,1))\n",
    "    hist = model.fit(x_train_cnn, y_train_cnn,\n",
    "                     validation_data=(x_val_cnn, y_val_cnn),\n",
    "                     epochs=30, batch_size=128,\n",
    "                     callbacks=callbacks_for(run_id), verbose=2)\n",
    "    _, test_acc = model.evaluate(x_test_cnn, y_test, verbose=0)\n",
    "    summaries.append(summarize_run(run_id, \"fmnist\", cfg, hist, test_acc))\n",
    "\n",
    "pd.DataFrame(summaries).to_csv(\"results/fmnist_summary.csv\", index=False)"
   ],
   "id": "90d67716bfec1390"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizing the validation accuracy for all configurations and stored the results",
   "id": "c862bc003529aa12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.DataFrame(summaries)\n",
    "df_sorted = df.sort_values(\"best_val_accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(df_sorted)), df_sorted[\"best_val_accuracy\"], marker=\"o\", linestyle=\"-\")\n",
    "plt.xticks(range(len(df_sorted)), df_sorted[\"run_id\"], rotation=90, fontsize=7)\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Model Configurations (shortened)\")\n",
    "plt.title(\"Fashion-MNIST: Validation Accuracy Across All Configurations\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/fmnist_all_configs_lineplot.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "top10 = df_sorted.head(10)\n",
    "top3 = top10.head(3).to_dict(orient=\"records\")\n",
    "with open(\"results/top3_fmnist.json\", \"w\") as f:\n",
    "    json.dump(top3, f, indent=2)\n"
   ],
   "id": "24100be96901d4e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retraining the CIFAR-10 dataset with top-3 architectures with hyperparameters in Fashion-MNiST dataset",
   "id": "b5c7b539c323c010"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "(x_train_c, y_train_c), (x_test_c, y_test_c) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train_c, y_test_c = y_train_c.flatten(), y_test_c.flatten()\n",
    "x_train_c, x_test_c = x_train_c.astype(\"float32\") / 255.0, x_test_c.astype(\"float32\") / 255.0\n",
    "\n",
    "x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(\n",
    "    x_train_c, y_train_c, test_size=0.1, random_state=42, stratify=y_train_c)\n",
    "\n",
    "retrain_results = []\n",
    "\n",
    "print(\"\\nRetraining top-3 Fashion-MNIST models on CIFAR-10\")\n",
    "\n",
    "for info in top3:\n",
    "    run_id, cfg = info[\"run_id\"], info[\"config\"]\n",
    "    print(f\"\\nTraining {run_id} on CIFAR-10 from scratch...\")\n",
    "\n",
    "    if run_id.startswith(\"mlp\"):\n",
    "        # Flatten CIFAR-10 for MLP input\n",
    "        x_train_c_flat = x_train_c.reshape((x_train_c.shape[0], -1))\n",
    "        x_val_c_flat   = x_val_c.reshape((x_val_c.shape[0], -1))\n",
    "        x_test_c_flat  = x_test_c.reshape((x_test_c.shape[0], -1))\n",
    "        model = build_mlp(cfg)\n",
    "        hist = model.fit(x_train_c_flat, y_train_c,\n",
    "                         validation_data=(x_val_c_flat, y_val_c),\n",
    "                         epochs=40, batch_size=128, verbose=2,\n",
    "                         callbacks=callbacks_for(run_id + \"_cifar\"))\n",
    "        _, test_acc = model.evaluate(x_test_c_flat, y_test_c, verbose=0)\n",
    "        retrain_results.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"config\": cfg,\n",
    "            \"test_accuracy\": float(test_acc)\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        model = build_cnn(cfg, input_shape=(32, 32, 3))\n",
    "        hist = model.fit(x_train_c, y_train_c,\n",
    "                         validation_data=(x_val_c, y_val_c),\n",
    "                         epochs=40, batch_size=128, verbose=2,\n",
    "                         callbacks=callbacks_for(run_id + \"_cifar\"))\n",
    "        _, test_acc = model.evaluate(x_test_c, y_test_c, verbose=0)\n",
    "        retrain_results.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"config\": cfg,\n",
    "            \"test_accuracy\": float(test_acc)\n",
    "        })\n",
    "\n",
    "pd.DataFrame(retrain_results).to_csv(\"results/cifar10_summary.csv\", index=False)"
   ],
   "id": "1db4ecf2885fde65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comparison plot (FMNIST vs CIFAR-10)",
   "id": "d6c4492196195e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged = pd.merge(df, pd.DataFrame(retrain_results), on=\"run_id\", how=\"inner\")\n",
    "if not merged.empty:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(merged[\"run_id\"], merged[\"final_test_accuracy\"], label=\"FMNIST\")\n",
    "    plt.bar(merged[\"run_id\"], merged[\"test_accuracy\"], alpha=0.7, label=\"CIFAR-10\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Top-3 Model Performance: Fashion-MNIST vs CIFAR-10\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/fmnist_cifar10_comparison.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n Done mate!!!!!!\")"
   ],
   "id": "10c5069cb2f750e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
